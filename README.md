﻿﻿﻿﻿# ICML2019-paperspapers in ICML 2019paperlist from : https://icml.cc/Conferences/2019/774 papersbest paper :I. Challenging Common Assumptions in the Unsupervised Learning of  Disentangled Representations 《挑战无监督解耦表示中的常见假设》作者：Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf, Olivier BachemII. Rates of Convergence for Sparse Variational Gaussian Process Regression《稀疏高斯过程回归变分的收敛速度》作者：DavidR. Burt1，Carl E. Rasmussen1，Mark van der Wilk2提名：Analogies Explained: Towards Understanding Word Embeddings作者：CarlAllen1，Timothy Hospedales1 ，来自爱丁堡大学。论文地址：https://arxiv.org/pdf/1901.09813.pdfSATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver作者：Po-WeiWang1，Priya L. Donti1 2，Bryan Wilder3，Zico Kolter1 4，分别来自卡耐基梅隆大学、南加州大学、Bosch Center for Artificial Intelligence。  论文地址：https://arxiv.org/pdf/1905.12149.pdfA Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks作者：Umut Şimşekli∗，L, event Sagun†, Mert Gürbüzbalaban‡，分别来自巴黎萨克雷大学、洛桑埃尔科尔理工大学、罗格斯大学。论文地址：https://arxiv.org/pdf/1901.06053.pdfTowards A Unified Analysis of Random Fourier Features作者：Zhu Li，Jean-François Ton，Dino Oglic，Dino Sejdinovic，分别来自牛津大学、伦敦国王学院。论文地址：https://arxiv.org/pdf/1806.09178.pdfAmortized Monte Carlo Integration作者：Adam Golinski、Yee Whye Teh、Frank Wood、Tom Rainforth，分别来自牛津大学和英属哥伦比亚大学。论文地址：http://www.gatsby.ucl.ac.uk/~balaji/udl-camera-ready/UDL-12.pdfSocial Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning作者：Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro A. Ortega, DJ Strouse, Joel Z. Leibo, Nando de Freitas，分别来自MIT媒体实验室、DeepMind和普林斯顿大学。论文地址：https://arxiv.org/pdf/1810.08647.pdfStochastic Beams and Where to Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement作者：Wouter Kool, Herke van Hoof, Max Welling，分别来自荷兰阿姆斯特丹大学，荷兰ORTEC和加拿大高等研究所(CIFAR)。论文地址：https://arxiv.org/pdf/1903.06059.pdftopic | n | name| note-|-|-|-ReinforcementLearning Theory| 1 orals| Safe Policy Improvement with Baseline Bootstrapping |unknown, **Looking forward to your contribution**ReinforcementLearning Theory| 2 orals|DeepMDP: Learning Continuous Latent Space Models for Representation Learning |(1) modeling the dynamics of an MDP, and (2) learning a useful abstract representation of the states of an MDP, **Looking forward to your contribution**ReinforcementLearning Theory| 3 orals|Learning from a Learner |IRL,, **no paper, Looking forward to your contribution**Deep RL 2| 1orals|Imitation Learning from Imperfect Demonstration|imitation learning , **Looking forward to your contribution**Deep RL | 1orals|SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning|model based , **Looking forward to your contribution**Deep RL | 1orals|Model-Based Active Exploration |model based, **Looking forward to your contribution**Deep RL | 2 orals|The Natural Language of Actions| grouping similar actions and utilizing relations between different actions.**Looking forward to your contribution**Deep RL |3 orals| Remember and Forget for Experience Replay |  (1) skips gradients computed from experiences that are too unlikely with the current policy and (2) regulates policy changes within a trust region of the replayed behaviors.**Looking forward to your contribution**Deep RL |4 orals| Trajectory-Based Off-Policy Deep Reinforcement Learning |  combining recent improvements in the reuse of off-policy data and exploration in parameter space with deterministic behavioral policies. **Looking forward to your contribution**Deep RL |5 orals| TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning | a flexible GM-based HRL framework which leverages efficient inference procedures to enhance generalisation and transfer power. **Looking forward to your contribution**Deep RL |6 orals| ELF OpenGo: an analysis and open reimplementation of AlphaZero | an open-source reimplementation of the AlphaZero algorithm. **Looking forward to your contribution**Deep RL |7 orals| Learning Latent Dynamics for Planning from Pixels |model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space.. **Looking forward to your contribution**Deep RL |8 orals| Calibrated Model-Based Deep Reinforcement Learning|model-based   **Looking forward to your contribution**learning theory &games | 1 orals| Regret Circuits: Composability of Regret Minimizers | **Looking forward to your contribution**learning theory &games | 2 orals | Game Theoretic Optimization via Gradient-based Nikaido-Isoda Function| about NE.  **Looking forward to your contribution**learning theory &games |3 orals| Stable-Predictive Optimistic Counterfactual Regret Minimization|  present the first CFR variant that breaks the square-root dependence on iteration .**Looking forward to your contribution**learning theory &games |4 orals| When Samples Are Strategically Selected|  unkown .**Looking forward to your contribution**learning theory &games |5 orals| Statistical Foundations of Virtual Democracy |  voting rule :  Borda count and its robustness .**Looking forward to your contribution**learning theory &games |6 orals| Optimal Auctions through Deep Learning |  Unknown .**Looking forward to your contribution** learning theory &games |7 orals| Learning to Clear the Market|  Unknown .**Looking forward to your contribution** learning theory &games |8 orals|Learning to bid in revenue-maximizing auctions|  Unknown .**Looking forward to your contribution**learning theory &games |9 orals|Open-ended Learning in Symmetric Zero-sum Games|   a geometric framework for formulating agent objectives in zero-sum games .**Looking forward to your contribution**learning theory &games |10 orals|Deep Counterfactual Regret Minimization|    "This is the first non-tabular variant of CFR to be successful in large games.".**Looking forward to your contribution**adversairal examples | 10 orals  |-|**Looking forward to your contribution**deeplearning theory |10 orals | -|**Looking forward to your contribution**